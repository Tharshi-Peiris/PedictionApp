# -*- coding: utf-8 -*-
"""object_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O4-GyhfmH3_IwPPv70wwO4tQBlgHOuKQ
"""

# Import necessary libraries
!pip install ultralytics

from ultralytics import YOLO
import os
import shutil
import random
from sklearn.model_selection import train_test_split
from google.colab import drive
from IPython.display import Image, display

# Mount Google Drive to access dataset
drive.mount('/content/drive')

# Define paths (update these based on your Google Drive structure)
dataset_path = '/content/drive/MyDrive/YOLO.v1i.yolov8/train'
image_dir = f'{dataset_path}/images'
label_dir = f'{dataset_path}/labels'
data_yaml = f'{dataset_path}/data.yaml'
test_image = f'{dataset_path}/Fresh Leaf (26).jpg'


# Create directories for train and validation splits
train_image_dir = f'{dataset_path}/images/train'
val_image_dir = f'{dataset_path}/images/val'
train_label_dir = f'{dataset_path}/labels/train'
val_label_dir = f'{dataset_path}/labels/val'

os.makedirs(train_image_dir, exist_ok=True)
os.makedirs(val_image_dir, exist_ok=True)
os.makedirs(train_label_dir, exist_ok=True)
os.makedirs(val_label_dir, exist_ok=True)

# Get list of images and labels
image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]
label_files = [f for f in os.listdir(label_dir) if f.endswith('.txt')]

# Ensure images and labels match
image_files = [f for f in image_files if f.replace(f'.{f.split(".")[-1]}', '.txt') in label_files]

# Shuffle the image files
random.shuffle(image_files)

# Split dataset (80% train, 20% validation)
train_images, val_images = train_test_split(image_files, test_size=0.2, random_state=42)

# Copy images and labels to respective train/val directories
for img in train_images:
    shutil.copy(os.path.join(image_dir, img), os.path.join(train_image_dir, img))
    label = img.replace(f'.{img.split(".")[-1]}', '.txt')
    shutil.copy(os.path.join(label_dir, label), os.path.join(train_label_dir, label))

for img in val_images:
    shutil.copy(os.path.join(image_dir, img), os.path.join(val_image_dir, img))
    label = img.replace(f'.{img.split(".")[-1]}', '.txt')
    shutil.copy(os.path.join(label_dir, label), os.path.join(val_label_dir, label))

# Create data.yaml file
yaml_content = f"""
train: {train_image_dir}
val: {val_image_dir}
nc: 6  # Replace with your number of classes
names: ['Anthracnose', 'Bacterial Wilt', 'Downy-mildew','Fresh','Gummy Stem Blight','Pawdery-mildew']  # Replace with your class names
"""
with open(data_yaml, 'w') as f:
    f.write(yaml_content)

# Load the pretrained YOLOv8s model
model = YOLO('yolov8s.pt')

# Train the model
model.train(
    data=data_yaml,
    epochs=50,
    imgsz=640,
    batch=16,
    name='yolov8s_custom',
    project='/content/drive/MyDrive/yolo_results',
    device=0  # Use GPU
)

# Perform inference on the test image
results = model.predict(
    source=test_image,
    save=True,
    project='/content/drive/MyDrive/yolo_results',
    name='inference'
)

# Display the output image with bounding boxes
output_image = os.path.join('/content/drive/MyDrive/yolo_results/inference', os.path.basename(test_image))
display(Image(filename=output_image))

# Import necessary libraries
from ultralytics import YOLO
from google.colab import drive
import os



# Path to the trained model weights (update if necessary)
model_path = '/content/drive/MyDrive/yolo_results/yolov8s_custom2/weights/best.pt'

# Load the trained model
model = YOLO(model_path)

# Save the model explicitly in PyTorch format (.pt)
output_dir = '/content/drive/MyDrive/yolo_results/saved_model'
os.makedirs(output_dir, exist_ok=True)
model.save(os.path.join(output_dir, 'yolov8s_custom.pt'))

# Optionally, export the model to other formats (e.g., ONNX)
model.export(format='onnx', dynamic=False)  # Exports to ONNX format
print(f"Model saved as PyTorch (.pt) in {output_dir}/yolov8s_custom.pt")
print(f"Model exported as ONNX in {output_dir}/yolov8s_custom.onnx")

# Install Ultralytics if not already installed
!pip install ultralytics

# Import necessary libraries
from ultralytics import YOLO
from google.colab import drive
import os
import shutil

# Define paths
model_path = '/content/drive/MyDrive/yolo_results/yolov8s_custom2/weights/best.pt'
save_dir = '/content/drive/MyDrive/yolo_results/saved_model'
os.makedirs(save_dir, exist_ok=True)

# Check if model exists
if not os.path.exists(model_path):
    raise FileNotFoundError(f"Model file not found at: {model_path}")

# Load YOLOv8 model
print("ðŸ”„ Loading YOLOv8 model...")
model = YOLO(model_path)
print("âœ… Model loaded.")

# Export to TFLite
print("ðŸ”„ Exporting to TFLite...")
exported_tflite_path = model.export(format='tflite', imgsz=640)
print(f"âœ… Export complete. File saved at: {exported_tflite_path}")

# Define target save path
target_tflite_path = os.path.join(save_dir, 'yolov8s_custom.tflite')

# Move the exported file
shutil.move(exported_tflite_path, target_tflite_path)
print(f"âœ… TFLite model moved to: {target_tflite_path}")

# Install required packages
!pip install opencv-python-headless

import tensorflow as tf
import numpy as np
import cv2
import matplotlib.pyplot as plt
from google.colab import files

# --- Upload Image ---
uploaded = files.upload()
image_path = next(iter(uploaded))  # Get the uploaded filename

# --- Load Image ---
img = cv2.imread(image_path)
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
original_height, original_width = img.shape[:2]
resized = cv2.resize(img_rgb, (640, 640))
input_data = np.expand_dims(resized, axis=0).astype(np.float32)  # No normalization here, model expects 0-255

# --- Load TFLite Model ---
tflite_model_path = '/content/drive/MyDrive/yolo_results/saved_model/yolov8s_custom.tflite'

interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
interpreter.allocate_tensors()

# Get input and output details
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# --- Run Inference ---
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])  # shape: (1, 8400, 10) or (1, 10, 8400)

# --- Process Output ---
# The output shape can vary, let's check and reshape if necessary
# Expected shape: (1, num_boxes, [xywh, objectness, class_probs]) or (1, [xywh, objectness, class_probs], num_boxes)
# Based on the error and common TFLite conversions, it's likely (1, 8400, 10) or transposed (1, 10, 8400)
if output_data.shape[-1] == 10: # Shape (1, num_boxes, 10)
    detections = output_data[0] # shape (num_boxes, 10)
elif output_data.shape[1] == 10: # Shape (1, 10, num_boxes)
    detections = output_data[0].T # Transpose to (num_boxes, 10)
else:
    raise ValueError(f"Unexpected model output shape: {output_data.shape}")

boxes = []
confidence_threshold = 0.3
class_names = ['Anthracnose', 'Bacterial Wilt', 'Downy-mildew','Fresh','Gummy Stem Blight','Pawdery-mildew'] # Match the names from data.yaml

for det in detections:
    # Assuming the 10 values are [x_center, y_center, width, height, objectness_score, class_prob_0, class_prob_1, ...]
    bbox = det[:4]
    objectness_score = det[4]
    class_probs = det[5:]

    # Calculate the final confidence score for each class
    final_scores = objectness_score * class_probs

    # Find the class with the highest score
    max_score_index = np.argmax(final_scores)
    max_score = final_scores[max_score_index]

    if max_score > confidence_threshold:
        # Convert center_x, center_y, width, height to x1, y1, x2, y2
        center_x, center_y, w, h = bbox

        # Convert normalized coordinates to original image coordinates
        x1 = int((center_x - w/2) * original_width / 640)
        y1 = int((center_y - h/2) * original_height / 640)
        x2 = int((center_x + w/2) * original_width / 640)
        y2 = int((center_y + h/2) * original_height / 640)

        # Ensure coordinates are within image bounds
        x1 = max(0, x1)
        y1 = max(0, y1)
        x2 = min(original_width, x2)
        y2 = min(original_height, y2)


        boxes.append((x1, y1, x2, y2, max_score, max_score_index))

# --- Apply Non-Maximum Suppression (NMS) ---
# You might need to install a library for NMS if not already available
# For simplicity, let's assume a basic NMS implementation or skip it for now
# NMS is crucial for removing overlapping duplicate detections.
# A common library is `tensorflow.image.non_max_suppression` or `torchvision.ops.nms`
# Since we are using TFLite interpreter, we'll use a numpy-based approach or a simpler method.

# Simple NMS (can be improved for better accuracy)
def non_max_suppression(boxes, confidence_threshold, iou_threshold):
    if len(boxes) == 0:
        return []

    # Sort boxes by confidence
    sorted_indices = np.argsort([b[4] for b in boxes])[::-1]

    keep_boxes = []
    while len(sorted_indices) > 0:
        current_index = sorted_indices[0]
        current_box = boxes[current_index]
        keep_boxes.append(current_box)

        # Calculate IoU with remaining boxes
        ious = []
        for i in sorted_indices[1:]:
            other_box = boxes[i]
            # Calculate intersection coordinates
            x1_inter = max(current_box[0], other_box[0])
            y1_inter = max(current_box[1], other_box[1])
            x2_inter = min(current_box[2], other_box[2])
            y2_inter = min(current_box[3], other_box[3])

            # Calculate intersection area
            intersection_area = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)

            # Calculate union area
            current_box_area = (current_box[2] - current_box[0]) * (current_box[3] - current_box[1])
            other_box_area = (other_box[2] - other_box[0]) * (other_box[3] - other_box[1])
            union_area = current_box_area + other_box_area - intersection_area

            # Calculate IoU
            iou = intersection_area / (union_area + 1e-6) # Add small epsilon to avoid division by zero
            ious.append(iou)

        # Filter out boxes with high IoU
        non_overlapping_indices = [i for i, iou in enumerate(ious) if iou < iou_threshold]
        sorted_indices = sorted_indices[1:][non_overlapping_indices]

    return keep_boxes

# Apply NMS with adjusted thresholds
iou_threshold = 0.4 # Intersection over Union threshold
nms_boxes = non_max_suppression(boxes, confidence_threshold, iou_threshold)


# --- Draw Bounding Boxes ---
# Use the original image dimensions for drawing
img_display = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Use original image for drawing

for x1, y1, x2, y2, conf, cls_id in nms_boxes:
    class_name = class_names[cls_id] if cls_id < len(class_names) else f"Class {cls_id}"
    label = f"{class_name} ({conf:.2f})"
    color = (0, 255, 0) # Green color for bounding box

    cv2.rectangle(img_display, (x1, y1), (x2, y2), color, 2)
    cv2.putText(img_display, label, (x1, y1 - 10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

# --- Show Result ---
plt.figure(figsize=(10, 10))
plt.imshow(img_display)
plt.axis('off')
plt.title("Detected Objects")
plt.show()

from IPython.display import Image, display
import os

# Define the path to the inference output directory and the test image name
inference_output_dir = '/content/drive/MyDrive/yolo_results/inference'
test_image_name = os.path.basename(test_image) # Use the same test_image variable from cell dBsGSjktu6aN

# Construct the full path to the output image
output_image_path = os.path.join(inference_output_dir, test_image_name)

# Display the output image
display(Image(filename=output_image_path))