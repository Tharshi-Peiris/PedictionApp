# -*- coding: utf-8 -*-
"""Multiclass_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HQq9-yofr2CIQ519LPT5C_yt1BJBLsc9
"""

# Import necessary libraries
import os
import random
import numpy as np
import tensorflow as tf
from collections import Counter
import matplotlib.pyplot as plt
from tensorflow.keras.applications import MobileNetV3Large
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

# Define dataset path
dataset_dir = '/content/drive/MyDrive/DataSet_New'

# Define image size, batch size, and number of classes
img_size = (224, 224)
batch_size = 32
num_classes = 6

# Load dataset from directory
dataset = tf.keras.utils.image_dataset_from_directory(
    dataset_dir,
    image_size=img_size,
    batch_size=None,
    shuffle=True,
    seed=42
)

# Convert dataset to numpy arrays
images = []
labels = []
for image, label in dataset:
    images.append(image.numpy())
    labels.append(label.numpy())

images = np.array(images)
labels = np.array(labels)

# Show original class distribution
class_names = dataset.class_names
class_counts = Counter(labels)
print("Original Class Distribution:")
for i, class_name in enumerate(class_names):
    print(f"Class '{class_name}': {class_counts[i]} images")

# Oversample to balance the classes
max_count = max(class_counts.values())
aug_images, aug_labels = [], []

for class_idx in np.unique(labels):
    class_images = images[labels == class_idx]
    class_labels = labels[labels == class_idx]

    num_to_add = max_count - len(class_images)

    if num_to_add > 0:
        indices = np.random.choice(len(class_images), num_to_add, replace=True)
        aug_images.extend(class_images[indices])
        aug_labels.extend(class_labels[indices])

# Combine original and augmented data
images = np.concatenate([images, np.array(aug_images)])
labels = np.concatenate([labels, np.array(aug_labels)])

# Shuffle the dataset
shuffled_indices = np.arange(len(images))
np.random.shuffle(shuffled_indices)
images = images[shuffled_indices]
labels = labels[shuffled_indices]

# New class distribution
print("\nBalanced Class Distribution:")
class_counts = Counter(labels)
for i, class_name in enumerate(class_names):
    print(f"Class '{class_name}': {class_counts[i]} images")

# Split dataset
train_images, temp_images, train_labels, temp_labels = train_test_split(
    images, labels, train_size=0.7, stratify=labels, random_state=42
)
val_images, test_images, val_labels, test_labels = train_test_split(
    temp_images, temp_labels, test_size=0.5, stratify=temp_labels, random_state=42
)

# One-hot encoding
train_labels = tf.keras.utils.to_categorical(train_labels, num_classes)
val_labels = tf.keras.utils.to_categorical(val_labels, num_classes)
test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)

# TensorFlow datasets
train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))
val_ds = tf.data.Dataset.from_tensor_slices((val_images, val_labels))
test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))

# Data augmentation
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal_and_vertical"),
    tf.keras.layers.RandomRotation(0.3),
    tf.keras.layers.RandomZoom(0.3),
    tf.keras.layers.RandomTranslation(height_factor=0.3, width_factor=0.3),
    tf.keras.layers.RandomContrast(0.2),
])

# Preprocessing
def preprocess(image, label):
    image = tf.image.resize(image, img_size)
    image = tf.keras.applications.mobilenet_v3.preprocess_input(image)
    return image, label

train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)
train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)
val_ds = val_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)
test_ds = test_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)

train_ds = train_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)
val_ds = val_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)
test_ds = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)

# Model
base_model = MobileNetV3Large(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)
x = Dropout(0.3)(x)
x = Dense(num_classes, activation='softmax')(x)
model = Model(inputs=base_model.input, outputs=x)

# Compile
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)

# Train without class_weight (because we oversampled)
history = model.fit(train_ds, epochs=50, validation_data=val_ds,
                    callbacks=[early_stopping, reduce_lr])

# Evaluate
train_loss, train_acc = model.evaluate(train_ds, verbose=0)
val_loss, val_acc = model.evaluate(val_ds, verbose=0)
test_loss, test_acc = model.evaluate(test_ds, verbose=0)

print(f"\nFinal Training Accuracy: {train_acc * 100:.2f}%")
print(f"Final Validation Accuracy: {val_acc * 100:.2f}%")
print(f"Final Test Accuracy: {test_acc * 100:.2f}%")

# Test predictions
test_images, test_labels = next(iter(test_ds.unbatch().batch(len(test_labels))))
test_predictions = model.predict(test_images)
test_pred_labels = np.argmax(test_predictions, axis=1)
test_true_labels = np.argmax(test_labels, axis=1)

# Metrics
accuracy = accuracy_score(test_true_labels, test_pred_labels) * 100
precision = precision_score(test_true_labels, test_pred_labels, average='weighted') * 100
recall = recall_score(test_true_labels, test_pred_labels, average='weighted') * 100
f1 = f1_score(test_true_labels, test_pred_labels, average='weighted') * 100

print(f"\nFinal Metrics on Test Set:")
print(f"Accuracy: {accuracy:.2f}%")
print(f"Precision: {precision:.2f}%")
print(f"Recall: {recall:.2f}%")
print(f"F1 Score: {f1:.2f}%")

# Save model
save_dir = '/content/drive/MyDrive/saved_model/'
if not os.path.exists(save_dir):
    os.makedirs(save_dir)
model.save(os.path.join(save_dir, 'mobilenetv3_plant_model.keras'))
print(f"Model saved to: {save_dir}")

# Convert to TFLite model
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save TFLite model
tflite_model_path = os.path.join(save_dir, 'mobilenetv3_plant_model.tflite')
with open(tflite_model_path, 'wb') as f:
    f.write(tflite_model)

print(f"TFLite model saved to: {tflite_model_path}")

# Save class labels to a text file (one label per line)
labels_path = os.path.join(save_dir, 'labels.txt')
with open(labels_path, 'w') as f:
    for label in class_names:
        f.write(label + '\n')

print(f"Labels saved to: {labels_path}")